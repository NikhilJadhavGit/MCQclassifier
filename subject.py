# -*- coding: utf-8 -*-
"""subject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JCPqgjB-BQW-E-kCU8I_uwWk2ErBVSg7
"""

import json
import numpy as np
from tensorflow.python.keras.preprocessing.text import Tokenizer
import tensorflow_hub as hub
import tensorflow as tf
import pandas as pd
from tensorflow.python.keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split

science='science.json'
history='history.json'
geography='geography.json'
book='book.json'

science_questions=json.load(open(science,'r'))
history_questions=json.load(open(history,'r'))
geography_questions=json.load(open(geography,'r'))
book_questions=json.load(open(book,'r'))

X=[]
Y=[]
answer=[]
for i in science_questions:
    X.append(str(i[0]))
    Y.append(int(i[1]))
for i in geography_questions:
    X.append(str(i[0]))
    Y.append(int(i[1]))
for i in history_questions:
    X.append(str(i[0]))
    Y.append(int(i[1]))
for i in book_questions:
    answer.append(str(i['question']))

import re
for i in range(len(answer)):
  answer[i]=(re.sub(r'Q[0-9]+.','',answer[i]))

from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

#Tokenize the sentences
tokenizer = Tokenizer(oov_token='$')

#preparing vocabulary
tokenizer.fit_on_texts(answer)

#converting text into integer sequences
x_tr_seq  = tokenizer.texts_to_sequences(X) 
ans_text_seq=tokenizer.texts_to_sequences(answer)
#padding to prepare sequences of same length
x_tr_seq  = pad_sequences(x_tr_seq, maxlen=100,dtype='float32')
ans_text_seq = pad_sequences(ans_text_seq, maxlen=100,dtype='float32')
y_tr_seq=to_categorical(Y,3)
size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding
print(size_of_vocabulary)

X_train, X_test, y_train, y_test = train_test_split(x_tr_seq, y_tr_seq, test_size=0.33, random_state=42)

from tensorflow.python.keras.models import *
from tensorflow.python.keras.layers import *
from tensorflow.python.keras.callbacks import *

model=Sequential()

#embedding layer
model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True)) 

#lstm layer
model.add(LSTM(128,return_sequences=True,dropout=0.2))

#Global Maxpooling
model.add(GlobalMaxPooling1D())

#Dense Layer
model.add(Dense(64,activation='relu')) 
model.add(Dense(3,activation='softmax')) 

#Add loss function, metrics, optimizer
model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=["acc"]) 

#Adding callbacks
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  
mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  

#Print summary of model
print(model.summary())

history = model.fit(X_train,y_train,epochs=20,steps_per_epoch=300,validation_data=(X_test,y_test),verbose=1,callbacks=[es,mc])

res=model.predict(ans_text_seq)

def pre(result):
  mapper={0:'science',1:'history',2:'geography'}
  result=list(result)
  return mapper[result.index(max(result))]

pre(res[2])

for i in range(len(res)):
  book_questions[i]['class']=pre(res[i])

with open("subject.json",'w') as file: 
  json.dump(book_questions,file,indent=4)



